{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 The engine of neural networks: Gradient-based optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하나의 레이어로 구성된 간단한 모델을 생각해 봅시다.\n",
    "\n",
    "```\n",
    "output = relu(dot(input, W) + b)\n",
    "```\n",
    "\n",
    "위 모델에서 W와 b는 레이어의 속성임. W와 b는 weight 또는 trainable parameter라고 함. weight는 모델이 데이터로부터 학습한 패턴에 대한 정보를 담고 있음\n",
    "\n",
    "처음에 이런 weight는 초기 값으로 임의 값을 갖고 있음. 임의 값으로 weight를 초기화하면 모델이 유용한 정보의 표현을 갖을 것으로 기대할 수 없지만 학습training을 통해서 weight를 점차 개선함.\n",
    "training loop이라고 하며 다음과 같은 과정의 반복을 통해 training loop를 통해 weight를 점진적으로 개선하게 됨\n",
    "1. training 샘플 x와 정답 y_true를 선택함\n",
    "2. y_pred의 예측을 얻기 위해 training sample x에 대해 모델을 실행함(forward pass)\n",
    "3. y_pred와 y_true 사이의 차이를 계산함(loss 계산)\n",
    "4. loss를 조금 줄이는 방향으로 weight를 수정함(update)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dc64fc6e19eed405ff4ebb7fc752bf301f3add8a871103af944071a777e7cfda"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
